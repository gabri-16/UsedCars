\chapter{MapReduce}

\section{Jobs}

\subsection{Job 1: Preprocessing}

Job execution and interfaces are described in figure \ref{fig:MR-job-1}.
 
Each raw record is read and parsed into a ``Car'', i.e. a custom ``Writable'' object, during map stage. 
Each ``Car'' stores data about:
\begin{itemize}
 \item Region
 \item Price
 \item Brand
 \item Fuel
 \item Odometer
\end{itemize}

The mapper output is a pair where the key is the default one used by Hadoop when reading text files and the value is the ``Car'' itself.

The reduce stage replaces the default key with a ``NullableWritable'' so that the whole job output is a set of records yet. Then, it publishes each record several times as specified by the replication factor.

Actually, the two phases could be collaped into one and reduce stage could be obmitted. However, a better separation of concerns between map related and reduce related responsabilities is preserved by keeping them separated.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{images/2-mapreduce/MR-job-1.png}
	\caption{Description of ``job 1: preprocessing'' workflow and interfaces.}
	\label{fig:MR-job-1}
\end{figure}

\subsection{Job 2a: Opi}

Job execution and interfaces are described in figure \ref{fig:MR-job-2a}.

The map stage input value is a ``Car'' encoded as text. The key is the Hadoop default one (it won't be used). The mapper calculates the OPI for a given car using price and odometer fields and gives a pair consisting of (brand, OPI) as output.

The reduce stage just calculates average values using single OPIs and reports it as output.
   
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{images/2-mapreduce/MR-job-2a.png}
	\caption{Description of ``job 2a: opi'' workflow and interfaces.}
	\label{fig:MR-job-2a}
\end{figure}

\subsubsection{Usage of combiners}

The job can be optimized by the usage of combiners. New job execution and interfaces are described in figure \ref{fig:MR-job-2a-combiner}.

The combiner stage calculates a partial average. Since the mean function is not associative, it is required the usage of a support variable so that a weighted mean can be calculated in reduce phase. Hence, interfaces between the different stages are fixed\footnote{Map interface is changed too since output from combiners and mappers must have the same format.} and a new custom ``Writable'' that stores info about partial mean and the number of items used when calculating such mean is created (``OpiAveragePair'').

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{images/2-mapreduce/MR-job-2a-combiner.png}
	\caption{Description of ``job 2a: opi'' workflow and interfaces with addition of combiners.}
	\label{fig:MR-job-2a-combiner}
\end{figure}   

\subsection{Job 2b: Region}

Job execution and interfaces are described in figure \ref{fig:MR-job-2b}.

The map stage input value is a ``Car' encoded as text. The key is the Hadoop default one for text files (it won't be used). If the fuel type is correct, the mapper generates as output a pair (region, brand) so that reduce stage can operate per region basis.

The reducer, given a region, firstly calculates the cardinality for each brand by collecting the input pairs. Then, it calculates the maximum and gives it as output in the form of (region, brand).

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{images/2-mapreduce/MR-job-2b.png}
	\caption{Description of ``job 2b: region'' workflow and interfaces.}
	\label{fig:MR-job-2b}
\end{figure}  

\subsubsection{Usage of combiners}

The job can be optimized by the usage of combiners. New job execution and interfaces are described in figure \ref{fig:MR-job-2b-combiner}.

The combiner stage calculates partial aggregations on brand frequencies. A support variable is added to save the counting so that partial results can be merged in reduce phase. Hence, interfaces between the different stages are fixed\footnote{Map interface is changed too since output from combiners and mappers must have the same format.} and a new custom ``Writable'' that stores info about brand and its frequency is created (``BrandQuantityPair'').

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{images/2-mapreduce/MR-job-2b-combiner.png}
	\caption{Description of ``job 2b: region'' workflow and interfaces with addition of combiners.}
	\label{fig:MR-job-2b-combiner}
\end{figure} 

\subsection{Job 3: Join}

Job execution and interfaces are described in figure \ref{fig:MR-job-3}.

The map stage takes as input pairs structured as (Text, Text). The source of a given pair can be from job 2a output or from job 2b output. By examinating a given input record, it is discovered its source and the pair is marked with a flag.
Since the join key will be the brand, the output of map stage is a pair (brand, (flag, region)), where flag and region are encapsulated into a ``JoinPair'' (a custom ``Writable'').

The reducer executes the join algorithm on input items and creates output pairs in the form (region, (brand, OPI)). 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{images/2-mapreduce/MR-job-3.png}
	\caption{Description of ``job 3: join'' workflow and interfaces.}
	\label{fig:MR-job-3}
\end{figure}  

\newpage
\section{Performance evaluation}

Performance are evaluated by varying the number of reducers, by enabling the use of combiners and by considering different replication factors.

Each casuistry result is the average value upon 3 runs. Results are shown in table \ref{table-mr-perf-100}.

\bigskip
\textit{Number of reducers}
\bigskip

As we can notice, the optimal number of reducers is 5\footnote{Actually, a more fine-grained analysis could be done by tuning the number of reducers on sigle jobs (instead of the same for each one). This could lead to find a more efficint setup but the number of combinations is quite huge and this argument will not be covered. }: too many reducers are an overkill due to task bootstrap and to the slender amount of data; on the other hand, too few reducers can't parallelize the work in the best way.

\bigskip
\textit{Combiners}
\bigskip

The usage of combiners improves performances independently from the number of reducers used. Obviously, the increase of performance decreases by incrementing the numbers of reducers. Indeed, the best increase is about 24\% when using just one reducer (parallelize aggregation work that would be totally sequential otherwise).

\bigskip
\textit{Replication factor}
\bigskip

By compairing results in tables \ref{table-mr-perf-100} and \ref{table-mr-perf-1}, we can see that execution times for query on the bigger dataset are slightly higher than the other ones. Hence, if we consider that there are two orders of magnitude between the size of the two datasets (8.2MB against 820MB) , it is evident how such an architecture is quite an overkill for small datasets.



\begin{table}[H]
  \centering
  \begin{tabular}{ |c c|c c c c c c c| } 
    \hline
    \multicolumn{2}{ |c| }{} & \multicolumn{7}{ c| }{Reducers} \\
    \multicolumn{2}{ |c| }{} & 1 & 2 & 5 & 6 & 8 & 10 & 100 \\
    \hline
    \multirow{2}{4em}{Combiner} 
    & No  & 292 & 244 & 223 & 235 & 251 & 280 & 1235 \\      
    & Yes & 221 & 215 & 210 & 226 & 228 & 270 & 1176 \\ 
    \hline
    \multicolumn{2}{ |c| }{Improvement} & 24.3\% & 11.8\% & 5.8\% & 3.8\% & 9.1\% & 3.5\% & 4.8\% \\
    \hline
  \end{tabular}
  \caption{Performance results expressed as elapsed time in seconds using \textbf{replication factor = 100}.}
  \label{table-mr-perf-100}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{ |c c|c c c c c| } 
    \hline
    \multicolumn{2}{ |c| }{} & \multicolumn{5}{ c| }{Reducers} \\
    \multicolumn{2}{ |c| }{} & 1 & 2 & 5 & 10 & 100 \\
    \hline
    \multirow{2}{4em}{Combiner} 
    & No  & 223 & 162 & 168 & 216 & 1400 \\      
    & Yes & 154 & 160 & 162 & 210 & 1210 \\ 
    \hline
  \end{tabular}
  \caption{Performance results expressed as elapsed time in seconds using \textbf{replication factor = 1}.}
  \label{table-mr-perf-1}
\end{table}